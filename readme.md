# New solution - quickstart guide

> All the steps outlined here are detailed further on down in the document.

1. Run docker-desktop
2. Run `./configure.ps1`
3. Open solution using Visual Studio, set docker-compose as Startup project and run the solution
4. Run `./create_migration.ps1 '' 'InitialMigration'`
5. Run `./migrate.ps1`
6. Run the solution again.
6. Go to http://localhost:5000/index.html

At this point only `.gitignore` has been committed locally. Now you can make some changes to the source code, push it to GitHub and get it deployed to your provisioned Azure resources:

1. Go to GitHub, create a new repository and add remote origin.
2. `git push -u origin master`
3. You can remove or update some of the endpoints and models if you like.
4. Go to Azure DevOps and create a new project.
5. Open `release_pipeline.yml` and set the value for `adoProject` variable. No need to touch anything if you are ok with the name and ADO project has the same name.
6. Set up [Azure Service Connection](#azure-service-connection). Copy the name from ADO to `release_pipeline.yml` to several properties named `azureSubscription` (or just name it `AzureConnection` in ADO).
7. `git checkout -b feature/initial-code-commit; git add *; git commit -m "Initial code commit."; git push -u origin feature/initial-code-commit`
8. Create and approve PR.
9. `git checkout master; git pull`
10. Now configure the pipelines on ADO. Add three new pipelines (`pr_pipeline`, `build_pipeline`, `release_pipeline`) based off of YAML files with the same name.
11. Configure the pipeline variables for `release_pipeline`. More on that in section [Release pipeline Database Migrations and Provisioning resources](#release-pipeline-database-migrations-and-provisioning-resources).
12. Create a new feature branch `git checkout -b feature/my-first-feature`. Do your work, create a PR and let the `pr_pipeline` do its work.
13. Approve PR. Let `build_pipeline` and `release_pipeline` do their work. You will probably need to open `release_pipeline` on the first run and approve some stuff.
14. Provision Azure resources - `release_pipeline` will do the work here as well.
15. Important security note: currently your API is not protected, even though everything is set up. You still need to apply `[Authorize]` attribute to your endpoint. It was done this way so as to simplify initial local development.

At this point you have a local environment and Azure fully set up, along with ADO pipelines ready deploy your code to a working AppService. Start working on your features!

# Before You Get Started

## Install a Docker host

E.g. Docker Desktop:

    choco install docker-desktop

## Configuration

### Set Configuration

Most of the stuff is in the `.env` file. This is a git ignored file, but it has the relevant structure in it. It gets generated by `./configure.ps1`. This script also prepopulates some values. It helps if you have the following beforehand:

- Service Bus connection strings (details below).
- Application Insights connection strings (details below).
- Azure AD authority and API identifier.
- Key Vault URI

### Local Database Configuration

This section describes how to connect to the database when running in a local Dockerized environment.

Database connection string for both `Api`, `WorkerServices` and `Migrations` projects is in the `.env` file. This was a deliberate choice, because then all the templated projects have a connection string automatically generated and in line with the name of the solution. You will notice there are two connection strings - `TestTemplate16DbConnection` is used by `Api` and `WorkerServices`, while `Migrations` project has a separate one `TestTemplate16Db_Migrations_Connection` because it is accessing the dockerized database from outside.

Username and password for the database are provided as default values through the `configure.ps1` script, but you can provide whatever values you want. 
Make sure you set the database-related variables before you run the solution for the first time, otherwise the database won't be configured properly. If you don't change those values before running the solution you will have to delete the `testtemplate16.sql` container and accompanying volumes. Even though the following is very unlikely, but if you do change `DbPassword` make sure the same value is set in `InitializeTestTemplate16Db.sql` for the login as well.

When you first run the solution a SQL script found in `src/InitializeTestTemplate16Db.sql` is executed, creating the database with an admin account (password in `DbAdminPassword`), login and user (`DbUser` and `DbPassword`). User is then assigned read, write and DDL roles.

Application is accessing the database as a `DbUser`/`DbPassword`, with a generated connection string found in `TestTemplate16DbConnection` and `TestTemplate16Db_Migrations_Connection`.

# Running The Application Locally

Make sure to set the `docker-compose` as the startup project. The application can be reached by default on `localhost:5000`. You can change this in the `docker-compose.yml`. Just go to `/index.html` to see the initial API.

At this point you have several things up and running:

- API (dockerized)
- Worker service (dockerized)
- Empty Sql Server database with a volume (dockerized)

Now it is time to create some tables in the database. From the root of your solution, first run `.\create_migration.ps1 '' '0001_Initial'` and then `./migrate.ps1`. Now you have to go to the SSMS and register your database server there. It is accessible on localhost, port 1433, with the username and password you set in your `.env` file under `DbUser` and `DbPassword`.

## Using Azure Resources from Visual Studio

API is configured to read from Key Vault even when ran locally. To be able to access an Azure resource such as Key Vault we have to log in to Visual Studio as a user having appropriate permissions. Therefore a user has to be created manually in Entra and assigned to a Users group (named as per release pipeline's `USERS_GROUP_NAME` variable). Depending on your security architecture, it might be enough to have only one such user and then assign it to all the relevant groups.

# Running The Application on Azure

## Security

Some Azure resources are accessed using connection strings (Application Insights, Service BUs) while other are accessed using Entra RBAC (SQL Server, Key Vault). Both SQL Server and Key Vault are accessed only by a Users group (named as per release pipeline's `USERS_GROUP_NAME` variable) - any Azure resource such as API have to be a member of this group to be able to access resources. Users group in turn has relevant roles to access SQL Server (`Reader`, `SQL Server Contributor`, `SQL Security Manager`) and Key Vault (`Key Vault Secrets Officer`).

As an example, API is running under an Managed Identity. This is used to connect to SQL and Key Vault. API uses connection strings to connect to Application Insights and Service Bus. API Managed Identity is assigned to Users group (named as per release pipeline's `USERS_GROUP_NAME` variable).

# Additional Stuff

## Branching strategy

Feature branches strategy is supported out of the box. This strategy expects all development to go through branches and committing directly to `master` is not allowed. Supported branches:

* `feature/`
* `fix/`

## Pipelines

### Naming the ADO project

`release_pipeline.yml` - `project` property on lines 43, 53, 63 should be the name of your ADO project.

### Azure YAML pipelines:

* `pr_pipeline`
* `build_pipeline`
* `release_pipeline`

All pipelines build and deploy all applications (`Api` and `WorkerServices`) in the solution.

When creating ADO pipelines, name them just like the files are named (minus the `.yml` suffix). Naming the pipelines same as the files is important because the `release_pipeline` is triggered by a successful `build_pipeline` run. If you decide to name your ADO pipelines differently, make sure you change two things in `release_pipeline.yml` - update `source` on line 8 and `definition` on line 40, 50, 60 to match the **build** pipeline name in ADO (if needed).

### Pipeline configuration

#### Azure Service Connection

In `release_pipeline.yml:72` there is an Azure subscription name (property name `azureSubscription` with initial value `AzureConnection`) - make sure the name is the same as what is in Azure.

If you are logged into ADO and Azure with different usernames, then you will need to go through additional steps to hook up ADO and Azure (essentially by creating a Service Principal and letting ADO know about it): more details [here](https://www.devcurry.com/2019/08/service-connection-from-azure-devops-to.html). The previous link describes the process nicely, but in case it is down try [this](https://learn.microsoft.com/en-us/azure/devops/pipelines/library/connect-to-azure?view=azure-devops#create-an-azure-resource-manager-service-connection-with-an-existing-service-principal) one.

Additionally, for the release pipeline to be able to register an API with Azure AD, you have to assign the Service Principal (above) an appropriate role (`Application Administrator`) on Entra, as described [here](https://stackoverflow.com/a/66204622/987827).

#### Release pipeline Database Migrations and Provisioning resources

`release_pipeline` deploys to resource group and resources based on pipeline variables:
* `ENVIRONMENT` - a moniker of your choosing to describe what environment you are deploying to. Can be any value (e.g. `dev`). Used to construct resource group name.
* `LOCATION` - must match names of regions Azure can understand, e.g. `westeurope`. Translated to a shorthand when constructing resource group name (e.g. `we`).
* `PROJECT_NAME` - a moniker of your choosing to denote the project. Used to construct resource group name.
* `SQL_ADMIN_USERNAME` - administrator username of your choosing.
* `USERS_GROUP_NAME` - group of users allowed to access to Azure resources (Key Vault, database...).

#### First deployment run

* `pr_pipeline` - on your first PR, the `pr_pipeline` will get triggered.
* `build_pipeline` - once you merge the PR, the build pipeline will get triggered. It is similar to `pr_pipeline`, except it uploads artifacts.
* `release_pipeline` - once the `build_pipeline` is done, `release_pipeline` will get triggered, but it will stall. You need to manually give a few permissions, it should start running from there on. This pipeline will provision all the resources, as described in [Azure](#azure) section. It also migrates the database ([Running from the pipeline](#running-from-the-pipeline)).

### Branches

**All** pipelines work with `master` branch . If you are using `main`, remember to do a search and replace.

## Project naming

All projects have a prefix `TestTemplate16` and pipelines latch onto that detail. If you want to start renaming projects, you should also do a search and replace across all the files in the solution. Be careful!

## Versioning

We are using semver and GitVersion. Each commit message gets a suffix (defined in `./githooks/prepare-commit-msg` and recognized in `GitVersion.yml`). `feature/` branch gets a suffix saying GitVersion should bump minor version. `fix/` branch gets a suffix saying GitVersion should bump patch version. Bumping major version needs to be done manually by tagging a commit. We do not embed the version in the assemblies yet. GitVersion depends on `--no-ff` merges to be able to deduce version successfully. Make sure your ADO project enforces this, do not allow developers to merge PRs differently! 

## Migrations

For migrations to work `.env` file must be properly set up with database credentials and connection string configured.

### Creating migrations

The below commands must be executed from solution root folder using Powershell. If this is the first migration in your project, execute:

    .\create_migration.ps1 '' '0001_Initial'

Every next migration must contain the name of the migration immediately preceeding it:

    .\create_migration.ps1 '0001_Initial' '0002_Second'

### Applying migrations

#### Running locally 

Command must be executed from solution root folder using Powershell. You will notice it is executing from a Docker container and Docker compose - the reason is this way there is only one `.env` which can be shared by all executeable projects in the solution (`Ąpi`, `Migrations`, `WorkerServices`).

    ./migrate.ps1

#### Running from the pipeline

As one of the last tasks, `release_pipeline` will execute a database migration using `Migrations` project. Even though deployed `Api` will connect to the database using MSI, migrations are still being executed from the pipeline directly and therefore require connecting to the database using the admin account. Sql admin account is configured using `SQL_ADMIN_USERNAME` pipeline variable and `SQL-ADMIN-PASSWORD` Key Vault secret. Pipeline is preconfigured to do this and no action is needed on your part.

# Azure

* `release_pipeline` will provision following resources on Azure, based off of config files and scripts you can find in `./deployment` folder:
  * SQL Server with a single database, using Entra-based admin account and Managed Identity. Does **not** use SQL contained users, but relies on Entra for all access management.
  * Application Insights
  * Service Bus
  * App Service - talks to SQL using Managed Identity.
  * Key Vault
    * Azure Connection service principal ADO is assigned to `Key Vault Secrets Officer` role scoped to this Key Vault.
    * Secrets created by the `release_pipeline`:
        * `SqlSaPassword` - generated by the pipeline (`secrets` task). This is the password used to deploy SQL Server to Azure. It is used only once, during the `release_pipeline` SQL Server provisioning step. Since SQL Server used Entra-only authentication, this secret isn't used afterwards, nor is it useful anymore. You can delete it from Key Vault afterwards if you want.
        * `SqlAdminPassword` - generated by the pipeline (`secrets` task). You can use these credentials to connect to deployed SQL Server. These are the credentials fed by the`release_pipeline` to `Migrations` project.
        * If you want additional secrets generated, edit `/deployment/secrets.ps1`.
  * Entra:
    * Register an API, create a Service Principal, expose the API and create one scope (`FullAccess`).
    * `sqlusers` group - App Service Managed Identity is assigned to this group by the release pipeline.
    * `sqladmin` user - Entra-based admin account, created by the release pipeline.